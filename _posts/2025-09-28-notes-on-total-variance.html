---
layout: post
title: "Notes on Total Variance"
date: 2025-09-28
categories: journal
---
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Author" content="Pavlo Mysak">
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2299.5">
  <style type="text/css">
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000}
    p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000; min-height: 14.0px}
    p.p4 {margin: 0.0px 0.0px 12.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000}
    p.p6 {margin: 0.0px 0.0px 0.0px 0.0px; text-align: center; font: 12.0px Times; -webkit-text-stroke: #000000}
    li.li4 {margin: 0.0px 0.0px 12.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000}
    span.s1 {font-kerning: none}
    span.s2 {font: 12.0px 'Lucida Grande'; font-kerning: none}
    span.s3 {font: 12.0px 'STIX Two Math'; font-kerning: none}
    span.s4 {font: 12.0px Times; text-decoration: underline ; font-kerning: none; color: #0000e3; -webkit-text-stroke: 0px #0000e3}
    span.s5 {-webkit-text-stroke: 0px #000000}
    span.s6 {font: 8.3px Times; font-kerning: none}
    span.s7 {font: 12.0px 'STIX Two Math'; -webkit-text-stroke: 0px #000000}
    span.s8 {font: 12.0px Symbol; font-kerning: none}
    span.s9 {font: 12.0px 'Hiragino Sans'; font-kerning: none}
    ul.ul1 {list-style-type: disc}
  </style>
</head>
<body>
<h1 style="margin: 0.0px 0.0px 0.0px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Notes on Total Variance</b></span></h1>
<p class="p2"><span class="s1">Pavlo Mysak</span></p>
<p class="p2"><span class="s1">2025-09-28</span></p>
<p class="p3"><span class="s1"></span><br></p>
<p class="p4"><span class="s1">Within my first month of working toward my masterâ€™s degree at Boston University, I noticed the same concept was popping up in totally different classes, but under different names and from different angles. It felt like dÃ©jÃ  vu, but math-y.</span></p>
<p class="p4"><span class="s1">That concept was total variance. And while the math itself is straightforward enough, what caught my attention is how different fields frame and interpret it. I havenâ€™t seen a post tying these perspectives together, so I figured Iâ€™d take a shot at formalizing this idea.</span></p>
<h1 style="margin: 0.0px 0.0px 16.1px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Law of Total Variance (Eveâ€™s Law)</b></span></h1>
<p class="p4"><span class="s1">Letâ€™s start with the probability theory side of things. The law of total variance tells us:</span></p>
<p class="p6"><span class="s1"><i>Var</i>(<i>Y</i>)</span><span class="s2">â€„</span><span class="s1">=</span><span class="s2">â€„</span><span class="s3">ğ”¼</span><span class="s1">[<i>Var</i>(<i>Y</i>|<i>X</i>)]</span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1"><i>Var</i>[</span><span class="s3">ğ”¼</span><span class="s1">(<i>Y</i>|<i>X</i>)]</span></p>
<p class="p4"><span class="s1">If youâ€™ve never seen this before it may look a little cryptic or suspicious, but itâ€™s called a <i>law</i> for a reason. And you donâ€™t just have to take my (or Eveâ€™s) word for it: with the law of iterated expectations (Adamâ€™s Law) and some algebra, you can prove it yourself. Turns out Adam &amp; Eveâ€™s contributions to probability theory are vastly underappreciated.</span></p>
<p class="p4"><span class="s1">Iâ€™ll skip the proof here (<a href="https://statproofbook.github.io/P/var-tot"><span class="s4">see references</span></a>), but the key idea is that the total variance of <i>Y</i> can be split into two pieces:<br>
- the <i>within-group variance</i>, </span><span class="s3">ğ”¼</span><span class="s1">[<i>Var</i>(<i>Y</i>|<i>X</i>)]<br>
- the <i>between-group variance</i>, <i>Var</i>(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>])</span></p>
<p class="p4"><span class="s1">If youâ€™ve ever seen ANOVA, this probably rings a bell. In that world, the whole point is to compare between-group variance to within-group variance: if the former is substantially larger, itâ€™s evidence that group means are likely different.</span></p>
<p class="p4"><span class="s1">Thatâ€™s one way to read this formula. But it can also be shown in a perspective that puts this decomposition in <a href="https://andrewcharlesjones.github.io/journal/epi-ali-uncertainty.html"><span class="s4">the language of uncertainty</span></a>. Specifically:</span></p>
<ul class="ul1">
  <li class="li4"><span class="s5"></span><span class="s1">The first term, </span><span class="s3">ğ”¼</span><span class="s1">[<i>Var</i>(<i>Y</i>|<i>X</i>)], corresponds to <b>aleatoric uncertainty</b>: the inherent randomness and noise built into the data-generating process. That is, itâ€™s the â€œnoise weâ€™re stuck with.â€</span></li>
  <li class="li4"><span class="s5"></span><span class="s1">The second term, <i>Var</i>(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>]), corresponds to <b>epistemic uncertainty</b>: uncertainty that comes from not knowing enough about the data-generating process. In other words, uncertainty that could shrink if we gathered more data or adjusted model parameters.</span></li>
</ul>
<p class="p4"><span class="s1">In Bayesian inference, this split isnâ€™t just a neat decomposition; itâ€™s baked into how we think about likelihoods and posteriors. Aleatoric uncertainty is the expected noise from the sampling model. Epistemic uncertainty is the variation induced by the posterior over parameters. And crucially, as we collect more data, epistemic uncertainty tends to fade away, which lines up perfectly with the intuition.</span></p>
<h1 style="margin: 0.0px 0.0px 16.1px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Biasâ€“Variance Tradeoff</b></span></h1>
<p class="p4"><span class="s1">Now letâ€™s switch gears to machine learning. A classic idea here is the <a href="https://mlu-explain.github.io/bias-variance/"><span class="s4"><i>biasâ€“variance tradeoff</i></span></a>. Youâ€™ve probably heard the high-level version before: models with too much flexibility tend to have high variance, while models that are too simple/rigid tend to have high bias. But letâ€™s actually derive where this decomposition comes from.</span></p>
<p class="p4"><span class="s1">Suppose we want to measure the error of a predictive model <i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>), where <i>D</i> is a dataset. A natural way to do this is to look at the squared deviation from the true signal <i>h</i>(<i>x</i>):</span></p>
<p class="p6"><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span></p>
<p class="p4"><span class="s1">Here:<br>
- <i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>) is our prediction given a particular dataset <i>D</i><br>
- <i>h</i>(<i>x</i>) is the â€œground truthâ€ or optimal prediction</span></p>
<p class="p4"><span class="s1">The trick is to rewrite this error by pivoting around the expected prediction, </span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]. In other words, add and subtract the same thing:</span></p>
<p class="p6"><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span></p>
<p class="p4"><span class="s1">Now expand it with (<i>a</i></span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1"><i>b</i>)</span><span class="s6"><sup>2</sup></span><span class="s2">â€„</span><span class="s1">=</span><span class="s2">â€„</span><span class="s1"><i>a</i></span><span class="s6"><sup>2</sup></span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1"><i>b</i></span><span class="s6"><sup>2</sup></span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1">2<i>ab</i>:</span></p>
<p class="p6"><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]]</span><span class="s6"><sup>2</sup></span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1">[</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1">2(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])(</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>))</span></p>
<p class="p4"><span class="s1">To turn this into a proper measure of error, we take the expectation over dataset <i>D</i> (think of it as averaging over many possible training samples):</span></p>
<p class="p6"><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>))</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[cross term]</span></p>
<p class="p4"><span class="s1">Hereâ€™s the nice part: with a bit of algebra, you can show the cross term vanishes. (Quick hint: itâ€™s the covariance between <i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)] and the constant term </span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>), which is zero.)</span></p>
<p class="p4"><span class="s1">So weâ€™re left with:</span></p>
<p class="p6"><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s2">â€…</span><span class="s1">+</span><span class="s2">â€…</span><span class="s1">[</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span></p>
<p class="p4"><span class="s1">And now we can name the two pieces:</span></p>
<ul class="ul1">
  <li class="li4"><span class="s7"></span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">] is the <b>variance</b></span></li>
  <li class="li4"><span class="s5"></span><span class="s1">[</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span><span class="s1"> is the <b>bias</b> squared</span></li>
</ul>
<p class="p4"><span class="s1">Thatâ€™s the biasâ€“variance decomposition. It tells us that model error isnâ€™t just one homogenous thing, rather itâ€™s the sum of two different sources.</span></p>
<p class="p4"><span class="s1">... doesnâ€™t this look awfully familiar?</span></p>
<h1 style="margin: 0.0px 0.0px 16.1px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>The Comparison</b></span></h1>
<p class="p4"><span class="s1">We know that:<br>
</span><span class="s3">ğ”¼</span><span class="s1">[<i>Var</i>(<i>Y</i>|<i>X</i>)]</span><span class="s8">â†’</span><span class="s1"> <b>aleatoric uncertainty</b><br>
<i>Var</i>(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>])</span><span class="s8">â†’</span><span class="s1"> <b>epistemic uncertainty</b><br>
</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s8">â†’</span><span class="s1"> <b>variance</b><br>
[</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span><span class="s2">â€„â†’â€„</span><span class="s1"><b>bias</b></span><span class="s6"><sup>2</sup></span></p>
<p class="p4"><span class="s1">How can we match up these terms based on similarity? Well, letâ€™s start with the easy one.</span></p>
<p class="p4"><span class="s1">Aleatoric uncertainty and the variance term are clearly long-lost cousins. Both are expectations of a squared deviation, and if you stare at the biasâ€“variance expression long enough, youâ€™ll notice </span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">] is literally the variance of <i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>). And what is <i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>), really, if not â€œ<i>Y</i> given <i>X</i>â€ in disguise?</span></p>
<p class="p4"><span class="s1">So:</span></p>
<p class="p6"><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[(<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)])</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s2">â€„</span><span class="s1">=</span><span class="s2">â€„</span><span class="s3">ğ”¼</span><span class="s1">[<i>Var</i>(<i>Y</i>|<i>X</i>)]</span></p>
<p class="p4"><span class="s1">Ergo, <b>variance = aleatoric uncertainty</b>. Nice and tidy.</span></p>
<p class="p4"><span class="s1">But what about the other half? Does that mean the bias term should line up with epistemic uncertainty?</span></p>
<p class="p4"><span class="s1">Letâ€™s think it through. Our bias,</span></p>
<p class="p6"><span class="s1">[</span><span class="s3">ğ”¼</span><span class="s6"><i><sub>D</sub></i></span><span class="s1">[<i>y</i>(<i>x</i>;</span><span class="s2">â€†</span><span class="s1"><i>D</i>)]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>h</i>(<i>x</i>)]</span><span class="s6"><sup>2</sup></span><span class="s1">,</span></p>
<p class="p4"><span class="s1">measures the systematic gap between the average prediction (across datasets) and the true signal <i>h</i>(<i>x</i>). Even if we trained on infinitely many datasets, if our model is misspecified, this term wonâ€™t vanish. Thatâ€™s the heart of bias: itâ€™s structural error baked into the model.</span></p>
<p class="p4"><span class="s1">Epistemic uncertainty, by contrast, is the part of our variance that <i>can</i> vanish with more information, at least in principle. It reflects what we donâ€™t yet know about the data-generating process, and with more data (or a more flexible model) it shrinks.</span></p>
<p class="p4"><span class="s1">Mathematically, epistemic uncertainty is written as</span></p>
<p class="p6"><span class="s1"><i>Var</i>(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>])</span><span class="s2">â€„</span><span class="s1">=</span><span class="s2">â€„</span><span class="s3">ğ”¼â€Š</span><span class="s1">[(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>])</span><span class="s6"><sup>2</sup></span><span class="s1">].</span></p>
<p class="p4"><span class="s1">At first glance, this doesnâ€™t look much like bias at all. In fact,</span></p>
<p class="p6"><span class="s3">ğ”¼</span><span class="s1">[(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>])</span><span class="s6"><sup>2</sup></span><span class="s1">]</span><span class="s2">â€„</span><span class="s1">â‰ </span><span class="s2">â€„</span><span class="s1">(</span><span class="s3">ğ”¼</span><span class="s1">[<i>Y</i>|<i>X</i>]</span><span class="s2">â€…</span><span class="s1">âˆ’</span><span class="s2">â€…</span><span class="s1"><i>Y</i>)</span><span class="s6"><sup>2</sup></span></p>
<p class="p4"><span class="s1">and unfortunately, Adam &amp; Eve canâ€™t rescue us from that algebra.</span></p>
<p class="p4"><span class="s1">But both terms are doing the same kind of job: theyâ€™re quantifying a gap between â€œwhat we knowâ€ and â€œwhatâ€™s actually out there.â€</span></p>
<p class="p4"><span class="s1">Bias squares the distance between our modelâ€™s average prediction and the truth. The epistemic term squares the distance between different possible truths (the conditional means) and the global average. Theyâ€™re not equal, but they rhyme.</span></p>
<p class="p4"><span class="s1">So the relationship is this:</span></p>
<ul class="ul1">
  <li class="li4"><span class="s5"><b></b></span><span class="s1"><b>Bias</b> makes the gap <i>explicit</i>: itâ€™s your modelâ€™s systematic error, locked into its parameters or functional form. This error wonâ€™t disappear unless the model itself gets better.</span></li>
  <li class="li4"><span class="s5"><b></b></span><span class="s1"><b>Epistemic uncertainty</b> makes the gap <i>probabilistic</i>: itâ€™s the spread in our beliefs about the process, which can shrink as data accumulates or the model grows more expressive.</span></li>
</ul>
<p class="p4"><span class="s1">Two sides of the same coin (one deterministic, the other probabilistic) but both about our ignorance of the true process.</span></p>
<p class="p4"><span class="s1">So hereâ€™s the mapping in a nutshell:</span></p>
<ul class="ul1">
  <li class="li4"><span class="s5"><b></b></span><span class="s1"><b>Variance</b> </span><span class="s9">â†”ï¸</span><span class="s1"> <b>aleatoric uncertainty</b> (noise weâ€™re stuck with).</span></li>
  <li class="li4"><span class="s5"><b></b></span><span class="s1"><b>Bias</b> </span><span class="s9">â†”ï¸</span><span class="s1"> <b>epistemic uncertainty</b> (ignorance we might reduce).</span></li>
</ul>
<p class="p4"><span class="s1">And once you see it this way, the two decompositions stop looking like separate ideas from two different courses, and more like the same story told in two different dialects.</span></p>
<h1 style="margin: 0.0px 0.0px 16.1px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>References</b></span></h1>
<p class="p4"><span class="s1">9</span></p>
<p class="p4"><span class="s1">Andrew Charles Jones (2022). <i>Epistemic and aleotoric uncertainty in statistical models</i> Technical Blog URL: <a href="https://andrewcharlesjones.github.io/journal/epi-ali-uncertainty.html"><span class="s4">https://andrewcharlesjones.github.io/journal/epi-ali-uncertainty.html</span></a>.</span></p>
<p class="p4"><span class="s1">Jared Wilbur &amp; Brent Werness (2021). <i>The Bias Variance Tradeoff</i>. MLU Explain Series URL: <a href="https://mlu-explain.github.io/bias-variance/"><span class="s4">https://mlu-explain.github.io/bias-variance/</span></a>.</span></p>
<p class="p4"><span class="s1">Joram Soch (2021). <i>Proof: Law of Total Variance</i>. The Book of Statistical Proofs, Proof #292. URL: <a href="https://statproofbook.github.io/P/var-tot"><span class="s4">https://statproofbook.github.io/P/var-tot</span></a>. DOI: <a href="file:///Users/pavlomysak/Downloads/10.5281/zenodo.4305949"><span class="s4">10.5281/zenodo.4305949</span></a>.</span></p>
<p class="p4"><span class="s1"><i>Note: These references are not the original sources of the concepts, but rather helpful resources I recommend to any reader not familiar with them.</i></span></p>
</body>
</html>
