---
title: "Multi-Linear-Reg-Car-Proj"
author: "Pavlo Mysak"
date: "2023-11-05"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Statement

Geely Auto, a Chinese automobile company, seeks to expand its presence in the US market by establishing a manufacturing unit to produce cars locally. They aim to gain a competitive edge by accurately predicting the prices of cars in the American market.

The company has collected a comprehensive dataset on various car attributes in the American market. The goal is to develop a predictive model to forecast the prices of cars, enabling Geely Auto to anticipate and set competitive prices for their vehicles.

# Business Goal

The objective is to build a robust predictive model that accurately estimates car prices based on a set of independent variables. This predictive tool will empower Geely Auto's management to anticipate market dynamics, allowing for informed decision-making in designing cars, devising business strategies, and adjusting pricing to meet specific targets. By leveraging the model's predictions, the company aims to proactively understand and adapt to the pricing dynamics of the American market, thereby enhancing their competitive positioning and strategic planning.

## Data

Import the data, remove unnecessary columns, split the data into training and validation sets and view a quick summary.

```{r, import data}
library(car)

data <- read.csv('/Users/pavlomysak/Downloads/archive-2/CarPrice_Assignment.csv', stringsAsFactors = T)
dt <- subset(data, select = -c(car_ID, CarName))

train <- dt[1:130,]
valid <- dt[-(1:130),]

summary(train)

```

## Correlation Matrix with Numeric/Integer Variables

Here we parse through the training dataframe to create a new dataframe including only numeric/int datatype columns. Using this new data frame, we can produce a correlation matrix to get a better idea of how our variables are related to one another. In the graphic below, we see quite a few linear relationships- it appears that a linear regression might be a good tool for this job!

```{r pressure, echo=FALSE}
library(GGally)

num_dt <- data.frame(matrix(ncol = ncol(train), nrow = nrow(train)))
colnames(num_dt) <- names(train)

for (col in 1:ncol(train)) {
  if (is.numeric(train[,col]) || is.integer(train[,col])) {
    num_dt[,col] <- train[, col]
  }
}

num_dt <- num_dt[, colSums(is.na(num_dt)) < nrow(num_dt)]

ggpairs(num_dt)

```

## Building our Initial Models

To fit our linear regression, we will be using the Ordinary Least Squares method. The OLS method is an optimization algorithm that minimizes the sum of squared vertical distances between observed data points and responses predicted by the linear model. These distances are also known as our residuals. 

In addition to using the OLS method to fit the model, we will also be using a bidirectional stepwise selection algorithm to construct an initial model. Stepwise selection approaches use a sequence of steps (forward, backward or both) to select the variables that maximize (or minimize) a certain model-fit criteria. The criteria we will be using is the Akaike Information Criterion or AIC. AIC is an estimator of prediction error and therefore suites the needs of our model application. It is important to note, however, that automated model selection algorithms can be seen as problematic because they are prone to over-fitting of data. In other words, the best AIC score does not always lead to the best model for real-world applications. Taking this into account, the stepwise selected model will only act as our initial model and further modifications will be made based on findings from exploratory data analysis (EDA) and intuition based off of domain knowledge. 


Let's build our initial model.

```{r}

null_model <- lm(data = train, price~1)
full_model <- lm(data = train, price~.)

model_1 <- step(null_model, scope = list(lower=null_model, upper=full_model),
     direction = 'both', trace = F)

summary(model_1)
```
## Model 1 Assesment

There are many variables in this model! The Adjusted R-Squared value looks good, but there are quite a few insignificant coefficients included in this model fit. 

Let's clean up a few of these factor variables next. Namely, CylinderNumber, CarBody and EngineType.
Our method to re-code these variables is to group them by Coefficient Estimate and Significance. 

For example, CylinderNumber can be re-coded into two groups: Negative and Insignificant. This is because Four, Five, Six and Twelve cylinder engines each have a negative coefficient (negatively impact price) while being statistically significant. The other two factors in this variable, three and two cylinder engines, are insignificant and have non-negative coefficients. 

We will repeat this process with the other aforementioned variables. Ideally, this will lower the complexity of our model and strengthen the predictive capabilities. 

```{r}
train$cylindernumber_recoded <- factor(with(train, ifelse(cylindernumber %in% c('five', 'four', 'six', 'twelve'),
                                                    'neg','insig')))
valid$cylindernumber_recoded <- factor(with(valid, ifelse(cylindernumber %in% c('five', 'four', 'six', 'twelve'),
                                                          'neg','insig')))

train$carbody_recoded <- factor(with(train, ifelse(carbody %in% c('hardtop','hatchback','wagon'),
                                      'neg','insig')))
valid$carbody_recoded <- factor(with(valid, ifelse(carbody %in% c('hardtop','hatchback','wagon'),
                                                   'neg','insig')))

train$enginetype_recoded <- factor(with(train, ifelse(enginetype %in% c('l','ohcf','rotor'),'insig', 
                ifelse(enginetype %in% c('dohcv','ohcv'), 'neg','pos'))))
valid$enginetype_recoded <- factor(with(valid, ifelse(enginetype %in% c('l','ohcf','rotor'),'insig', 
                                                      ifelse(enginetype %in% c('dohcv','ohcv'), 'neg','pos'))))

```
# Re-Running

Let's rerun model 1 with these re-coded values and assess our results.
```{r}
model_2 <- lm(formula = price ~ enginesize + cylindernumber_recoded + enginetype_recoded + 
     stroke + compressionratio + peakrpm + carbody_recoded + carwidth + 
     enginelocation + curbweight + carlength + aspiration, data = train)
summary(model_2)
```
# Model 2 Assessment

It appears that our Adjusted R-Squared Value decreased, but most of our variables are now significant!
Let's return back to our correlation matrix from earlier and see if there are any strong variables we are missing.

When consulting with the correlation matrix, strong linear relationships with price are seen with the following variables:
wheelbase, carlength, carwidth, curbweight, enginesize, boreratio, horsepower, citympg, highwaympg

Stroke and CompressionRatio, both seen in our model, have very poor linear relationships with price. However, Stroke is quite statistically significant. 

Let's examine the Model 2 variables and their relationships to Price.

```{r}
library(tidyr)
library(dplyr)

train_subset_long <- tidyr::gather(select(train, c('price', 'enginesize', 'cylindernumber_recoded', 'enginetype_recoded', 
                                                   'stroke', 'compressionratio', 'peakrpm', 'carbody_recoded',  'carwidth', 
                                                   'enginelocation', 'curbweight', 'carlength', 'aspiration')), key = "variable", value = "value", -price)

# Plotting the scatterplots using ggplot and facet_wrap
ggplot(train_subset_long, aes(x = value, y = price)) +
  geom_point() +
  facet_wrap(~ variable, scales = 'free') +
  labs(y = "Price")
```

Let's construct a 3rd model using a collection of variables from Model 2 and the variables we've identified to be strong from the correlation matrix. 

Model 3 formula = price ~ enginesize + cylindernumber_recoded + enginetype_recoded + 
    boreratio + horsepower + carbody_recoded + carwidth + 
    enginelocation + curbweight + stroke

```{r}
model_3 <- lm(formula = price ~ enginesize + cylindernumber_recoded + enginetype_recoded + 
    boreratio + horsepower + carbody_recoded + carwidth + 
    enginelocation + curbweight + stroke, data = train)
summary(model_3)
```

After some trial and error of this third model, we have a formula that includes all statistically significant variables (excluding the intercept coefficient) and an Adjusted R-Squared value of 0.933.

## Interpretation of Model 3

Generally, simple linear models (including only main effects) follow the formula:\n

Y = B0 + B1*X1 + B2*X2 + ... + Error \n

where Y is our Dependent Variable, B0 is our Y-Intercept, B(n) are our Slope Coefficients and X(n) are our Independent Variables. This formula will help us with fitting predictions to this model.


In our model:

1 unit increase in engine size increases the price of a vehicle by $100. \n

1 unit increase in bore ratio decreases the price of a vehicle by $4782. \n

1 unit increase in horese power increases the price of a vehicle by $34. \n

1 unit increase in car width increases the price of a vehicle by $500. \n

1 unit increase in curb weight increases the price of a vehicle by $6. \n

1 unit increase in stroke decreases the price of a vehicle by $4826. \n

If a vehicle has a four, five, six or twelve cylinder engine, the price is decreased by $9032 \n

If a vehicle has a DOHCV or OHCV engine, the price is decreased by $5772 \n

If a vehicle has a DOHC, OHC engine, the price is increased by $4042 \n

If a vehicle has a Hardtop, Hatchback or Wagon body, the price is decreased by $963 \n

If a vehicle has its engine located in the rear, the price is increased by $13551 \n


# Evaluating the Validity and Performance of our Model

## Adjusted R-Squared
As seen above, the Adj. R-Squared value for our final model is 0.933. This means that our model can account for about 93.3% of the observations within our data. While keeping in mind that 0.7 is the industry standard for linear models, this is pretty good! However, Adj. R-Squared values do not tell the whole story and can be misleading.

## RMSE
Root-Mean-Squared Error is another method of evaluating regression model performance. 
It measures the difference between the model's predicted values and our actual (observed) values.
RMSE can also be seen as the standard deviation of the residuals associated with our model or the average error of predictions. The lower the RMSE, the better our model fits the data. 

Let's calculate the in-sample and out-of-sample RMSE, respectively. Our out-of-sample RMSE is higher than our in-sample, which is to be expected, but only by 577 Units (dollars). Because the difference is fairly small, only some over-fitting is present in our model. Overall, our out-of-sample predictions may be about $2877.56 off from the real price.

```{r}
sqrt(mean((train$price-predict(model_3, train))^2)) # in-sample

sqrt(mean((valid$price-predict(model_3, valid))^2)) # out-of-sample
```

# Predictions

Geely Auto has envisioned a revolutionary sedan that they've determined would be popular in the American auto market. The company's aspiration is to craft a car that encapsulates superior performance, innovative design, and competitive pricing.

With this ambition in mind, Geely Auto has meticulously designed a prototype featuring specific attributes:

Enginesize: 150

Cylindernumber: four

Enginetype: ohcv

Boreratio: 3.4

Horsepower: 195

Carbody: sedan

Carwidth: 65

Enginelocation: front

Curbweight: 2515

Stroke: 3.2


According to our model, Geely Auto should price this new sedan at $9652 to adequately compete in the American auto market. This data, in conjunction with the costs associated with producing the vehicle will be crucial in determining the final consumer price of the vehicle.

```{r}
new_car <- data.frame(enginesize = 150,
                      cylindernumber_recoded = 'neg',
                      enginetype_recoded = 'neg',
                      boreratio = 3.4,
                      horsepower = 195,
                      carbody_recoded = 'insig',
                      carwidth = 65,
                      enginelocation = 'front',
                      curbweight = 2515,
                      stroke = 3.2)

predict(model_3, newdata = new_car)

```

## Residual Analysis
the mean of residuals is approximately 0, however, the residuals do not appear to be randomly distributed around the horizontal axis. This suggests that a non-linear model may be a more appropriate fit for our data. 
```{r}
mean(model_3$residuals)
residualPlot(model_3)
```
Do our residuals follow a normal distribution? According to the Shapiro-Wilk test, they do not follow a normal distribution, but if we generate a histogram, they appear to be fairly normal, aside from the outliers on the right tail. If we remove these extreme values, we see that the residuals do indeed follow a normal distribution according to the Shapiro-Wilk test for normality. 

```{r}
shapiro.test(model_3$residuals)
hist(model_3$residuals)

shapiro.test(model_3$residuals[model_3$residuals < quantile(model_3$residuals, 0.999)]) # removing upper outliers
```

## Heteroscedasticity

Heteroscedasticity is an issue that occurs when the variance of the predicted variable changes over different values of the independent variable. The existence of heteroscedasticity is a major concern in regression analysis and the analysis of variance, as it invalidates statistical tests of significance that assume that the modelling errors all have the same variance. We can check for Heteroscedasticity with the Breusch-Pagen Test (NCV Test).

According to our test, Heteroscedasticity is present in our model, however, because we are using the OLS method in fitting of our model, our predictions will remain unbiased and consistent. (Although, they will no longer qualify to be the Best Linear Unbiased Estimators because they are no longer efficient).

```{r}
ncvTest(model_3)
```

## Multicollinearity

there seems to be multicollinearity present in a few variables within our model. To measure multicollinearity, we use the Variance Inflation Factor Test. When running a VIF Test, we are essentially making each variable a dependent variable and regressing it against every other variable. 

At a threshold GVIF of 5, our problem variables are: EngineSize, EngineType, CarWidth,Curbweight

Generally, multicollinearity means that our coefficients are not uniquely established in our model. This can be an issue when the purpose of your model is to explain how your independent variables interact with your dependent variable, but for predictive purposes, it is not much of an issue. 

```{r}
vif(model_3)
```

# Conclusions

In our final linear model, the variables used to predict car prices are engine size, cylinder number, engine type,
bore ratio, horsepower, car body type, car width, engine location, 
curb weigh and stroke. 
    
There are some issues found in our model evaluation process that degrade the validity of our model. These issues include multicollinearity, heteroscedasticity and a not perfectly random residual plot. While these complications do deflate our confidence in the explanatory power of our model and some statistical significance of coefficients, they do not have a fatal impact on the predictive power of our model. 

### Referencesn used

https://cran.r-project.org/web/packages/olsrr/vignettes/heteroskedasticity.html#:~:text=Consequences%20of%20Heteroscedasticity,predictions%20will%20be%20inefficient%20too.

https://www.immagic.com/eLibrary/ARCHIVES/GENERAL/WIKIPEDI/W120529O.pdf

https://bookdown.org/max/FES/greedy-stepwise-selection.html

https://medium.com/geekculture/akaike-information-criterion-model-selection-c47df96ee9a8
